# One‑Page CV (Oct 2025)

**Sagi Levinas** · Weeki Wachee, FL (Remote) · linkedin.com/in/sagilevinas
**Software Engineer — Python • CI/CD • Ansible • Healthcare**

**Summary**
Engineer focused on **Python**, **CI/CD automation**, and **spec‑driven orchestration**. Built reusable **GitHub Actions + Ansible** pipelines for remote test execution and **Allure** reporting; delivered **FHIR/HL7**‑aligned features and data pipelines (AWS RDS → Snowflake).

**Core Skills**
Python · GitHub Actions · Ansible · Docker/Compose · (basic) Kubernetes · Pytest · Allure · SQL/PostgreSQL · ETL · Playwright · Node/NestJS · React/Vue · REST (OpenAPI)

**Experience**
**XPLG — Python/CI Consultant (Contract)** · Remote · **May–Dec 2025**
• Built **spec‑driven CI** chaining infra setup → remote **pytest** → **Allure** publish using reusable **GitHub Actions + Ansible**.
• Inventory‑scoped runs; secure secrets (SSH agent, `.env` via GitHub Secrets).
• **Artifact contracts** (`raw-results-*.tgz`), triage‑friendly retention.
• Unified **ensure‑infra/compose**: optional Python/Docker install, repo clones, dir creation, file upload, compose bring‑up.
• Operated **self‑hosted Allure** on a separate VM; CI publishes with history.
**Impact:** ↓ flake/rerun **[X]%** · ↓ mean setup **[Y] min** · ↓ time‑to‑signal **[Z]%**.

**VisionTree (Brainlab) — Advanced Software Engineer** · FL · **Oct 2021–Jul 2024**
• Delivered features aligned to **FHIR/HL7** for a patient‑reported outcomes platform.
• Built **ETL** (AWS RDS → Snowflake) + complex SQL for compliance analytics.
• Migrated legacy ColdFusion to **NestJS (TypeScript)**; added automated tests.
• Weekly SQL insights to reduce heavy queries and improve reliability.

**Earlier**
**STRS Energy (Aniam)** — Eng/Operations Mgr (2018–2021): led electro‑mechanical product; cut mfg costs via process redesign.
Project Manager (2015–2016): managed ~$10M hospital systems install; led subcontractors.
Engineering Consultant (2012–2016): installs/QA; iterative improvements.

**Education**
B.S. Mechanical Engineering — Tel Aviv University
Real Estate Appraisal (certificate) — Ariel University

**Projects**
Public, redacted **Spec‑Driven CI + Ansible** template (readme + workflows + playbooks + Allure doc).

---

# Cover Letter — Covera Health (Cloud/Infra Software Engineer)

Dear Covera Hiring Team,

I’m a Python‑focused software engineer who builds reliable, secure CI/CD systems for clinical software. Most recently, I led a transition to a **spec‑driven GitHub Actions + Ansible** framework that provisions infrastructure, executes containerized tests on a remote VM, and publishes **Allure** reports with strict artifact contracts. Earlier at **VisionTree (Brainlab)**, I delivered features aligned to **FHIR/HL7** and built an **AWS RDS (PostgreSQL) → Snowflake** ETL with complex SQL for patient‑reported outcomes analytics.

I enjoy turning brittle pipelines into **composable, observable** delivery systems. My recent work includes inventory‑scoped Ansible runs, secure secret handling (ephemeral SSH agent, GitHub Secrets), and a **self‑hosted Allure server** on a separate VM with history retention. I’m comfortable with Docker/Compose and can translate patterns to **Kubernetes** (Helm/Kustomize basics). I write clean, testable **Python**, and I care about data quality, security, and clear interfaces across Product, Clinical, and Data Science teams.

Covera’s mission—using clinical intelligence to improve diagnostic accuracy—resonates with me. I’d value the chance to contribute to your platform’s **cloud‑native services, CI/CD, and data workflows** and to partner closely with stakeholders to reduce misdiagnoses at scale.

Thank you for your time—looking forward to speaking.

Sagi Levinas
linkedin.com/in/sagilevinas

---

# How to use these effectively

**One‑pager**
• Upload as your primary resume on job sites.
• Tailor 2–3 bullets per role to the JD keywords (Python, Kubernetes, CI/CD, PostgreSQL, HL7/FHIR).
• Keep the X/Y/Z metrics at the top; quantify as soon as you have estimates.

**Cover letter**
• Paste into the application portal (or email).
• Add a single line pointing to your public **CI + Ansible** template repo.
• If a referral shares it internally, ask them to include the one‑liner about Allure server and spec‑driven orchestration.

**Portfolio**
• Publish the **redacted CI template repo**. Add a `tests/test_smoke.py` and CI badge.
• Screenshot an Allure report (no PHI), include in the README.

---

# 30‑day plan to get hired

**Week 1 — Prep & Materials**

1. Publish the **Spec‑Driven CI + Ansible** repo (public).
2. Finalize metrics (X/Y/Z) from recent runs; update one‑pager + LinkedIn.
3. Add 3–4 LinkedIn feature cards: Allure screenshot, CI diagram, repo link, brief write‑up.

**Week 2 — Targeted Applications (10–15 roles)**
4) Prioritize **health‑tech & platform** roles (Python + CI/CD + cloud).
5) For each role: tailor the one‑pager bullets and send a short, JD‑mirrored cover letter.
6) Log each app (role, date, contact, status). Follow up at **T+7 days**.

**Week 3 — Networking & Signals**
7) Identify 2–3 **Covera** engineers/PMs on LinkedIn; send a tight 4‑line note with your repo link and 1 quant metric.
8) Ask **1–2 prior colleagues** for a quick “results‑focused” LinkedIn recommendation.
9) Post a short thread on LinkedIn: *“Spec‑driven CI for remote pytest + Allure in health‑tech”* (diagram + repo link).

**Week 4 — Interview Readiness & Momentum**
10) Prepare 3 **STAR stories** (modularizing CI, reporting loop with Allure, FHIR/HL7 feature + ETL).
11) Practice a 12‑minute whiteboard: *“From Compose to K8s (Helm) in a CI pipeline.”*
12) Keep the pipeline full (5 new apps this week); follow up on all pending with a polite nudge + fresh signal (new README section, added tests).

**Ongoing**
• Track applied → screening → tech → onsite → offer.
• After each interview, send a same‑day note summarizing how you’d start in week one (clear next steps).


---

Perfect — I created both deliverables and laid out how to use them + a concrete job-hunt plan.

## Downloads

* One-pager CV: [PDF](sandbox:/mnt/data/Sagi_Levinas_OnePage_CV_Oct_2025.pdf) · [Markdown](sandbox:/mnt/data/Sagi_Levinas_OnePage_CV_Oct_2025.md)
* Covera-specific cover letter: [PDF](sandbox:/mnt/data/Sagi_Levinas_Cover_Letter_Covera_Oct_2025.pdf) · [Markdown](sandbox:/mnt/data/Sagi_Levinas_Cover_Letter_Covera_Oct_2025.md)
* (From earlier) Full CV: [PDF](sandbox:/mnt/data/Sagi_Levinas_CV_Oct_2025.pdf)
* Public repo scaffold to publish: [spec-driven-ci-ansible-template.zip](sandbox:/mnt/data/spec-driven-ci-ansible-template.zip)

## How to use them (quick playbook)

* **One-pager CV** → upload as your default resume. For each application, tweak 2–3 bullets to mirror the JD’s keywords (Python, CI/CD, Kubernetes, AWS/GCP, PostgreSQL, HL7/FHIR). Keep your X/Y/Z metrics updated.
* **Cover letter** → paste into the portal or email. Add one line pointing to your public “Spec-Driven CI + Ansible” repo once it’s on GitHub.
* **Full CV** → bring to recruiter calls or submit when they ask for a detailed version.
* **Repo** → publish publicly; add `tests/test_smoke.py`, a CI badge, and a redacted Allure screenshot (no PHI). Link it in your LinkedIn Featured section.

## 30-day “get hired” plan (concise)

**Week 1 (Materials):**

1. Publish the repo; add smoke test + badge.
2. Pull 3 metrics from recent runs (flake rate ↓, setup time ↓, time-to-signal ↓).
3. Update LinkedIn headline and About; add Featured: repo, Allure screenshot, CI diagram, full CV.

**Week 2 (Applications, 10–15 roles):**
4) Target health-tech + platform (Python/CI/CD/cloud).
5) For each role: tailor one-pager + cover letter; apply.
6) Track apps; follow up **7 days** later with a short note + repo link.

**Week 3 (Networking):**
7) Message 2–3 Covera engineers/PMs: 4-line note, repo link, 1 metric.
8) Ask 1–2 colleagues for a LinkedIn recommendation focused on outcomes.
9) Post a LinkedIn mini-thread: “Spec-driven CI for remote pytest + Allure in health-tech.”

**Week 4 (Interviews & momentum):**
10) Prep 3 STAR stories (modular CI, Allure reporting loop, FHIR/HL7 + ETL).
11) Rehearse a 12-minute whiteboard: “Compose → Helm on K8s in CI.”
12) Keep pipeline full (apply to 5 new roles); nudge pending apps with a fresh signal (e.g., repo improvement).

If you’d like, I can also:

* add a tiny `tests/test_smoke.py` + Makefile to the repo zip,
* generate a Covera recruiter outreach DM,
* or tailor the one-pager for 2–3 other target roles next.

---

Short answer: yes—this is very close and absolutely attachable after a couple of tiny fixes. Here are the **surgical edits** I recommend before you hit submit:

### Must-fix (typo/clarity)

* In XPLG, first bullet: change
  “**self-hosted GitRunner** on a remote VM to run CI workflows **And** a self-hosted Allure service…” →
  “**self-hosted GitHub Actions runner** on a remote VM and a **self-hosted Allure service** on a separate VM; wired CI to publish results and generate reports with history.”
* Keep “`fromJSON(...)`” styled as code (backticks) where it appears. 

### Nice-to-have (1 minute)

* Swap the first two XPLG bullets so **the spec-driven CI orchestrator** leads. It maps directly to Covera’s cloud/infra focus. 
* In **Technical Snapshot**, upgrade “basic Kubernetes” to “**Kubernetes (Helm basics)**” *if accurate* and surface **PostgreSQL** earlier (it’s in the JD). You already list PostgreSQL further down—mirroring it higher improves keyword match. 
* If you’ve touched Terraform at all, add “Terraform (introductory)” under Infra; otherwise skip (don’t inflate).
* Add **one quantified line** under XPLG (even conservative estimates): “Impact: reduced flake/rerun rate ~10–20%; setup time −5–10 min; time-to-signal to Allure −20–30%.”

### Verdict & how to attach for Covera

* **Attach the one-page CV** for the application (ATS-friendly and focused), and paste the **Covera cover letter** in the portal. Keep this 2-page CV ready for recruiter follow-ups.
* In the cover letter or “additional info,” include a link to your **public spec-driven CI + Ansible template repo**.
* Optional but great: add a single Allure screenshot (no PHI) to your LinkedIn Featured and reference it.

If you’d like, I can regenerate your PDF with the tiny wording fix + ordering tweak right now so it’s plug-and-play.

----


# Spec-Driven CI + Ansible — Public Repo Scaffold

Copy these files into a new GitHub repository (public/redacted). All secrets remain in **GitHub Secrets**. Replace placeholders like `<INVENTORY_HOST>` and `<ALLURE_VM_HOST>`.

---

## Repository tree

```
spec-driven-ci-ansible-template/
├── .github/
│   └── workflows/
│       ├── ci-orchestrator.yml
│       ├── ensure-infra-compose.yml
│       ├── run-pytest.yml
│       └── push-results-to-allure.yml
├── .ansible/
│   ├── inventory/
│   │   └── hosts.ini
│   └── playbooks/
│       ├── load_infra_spec.yml
│       ├── load_pytest_spec.yml
│       ├── ensure_env.yml
│       ├── create_directories.yml
│       ├── upload_files.yml
│       ├── ensure_compose.yml
│       ├── run_pytest.yml
│       └── push_allure_results.yml
├── .ci/
│   ├── infra/infra.compose.template.yml
│   ├── pytest/pytest.template.yml
│   └── allure/inputs-push-results.yml
├── docs/
│   └── allure-server-setup.md
├── .gitignore
├── LICENSE
└── README.md


```

---

## `.github/workflows/ci-orchestrator.yml`

```yaml
name: CI Orchestrator — Ensure → Test → Publish
on:
  push:
    branches: [ ci/spec-driven-template ]

jobs:
  ensure-infra-compose:
    uses: ./.github/workflows/ensure-infra-compose.yml
    with:
      infra_path: ./.ci/infra/infra.compose.template.yml
    secrets:
      ssh-key: ${{ secrets.TESTER_SSH_KEY }}

  run-pytest:
    needs: ensure-infra-compose
    uses: ./.github/workflows/run-pytest.yml
    with:
      spec_path: ./.ci/pytest/pytest.template.yml
    secrets:
      ssh-key: ${{ secrets.TESTER_SSH_KEY }}
      TESTER_ENV_FILE_CONTENTS: ${{ secrets.TESTER_ENV_FILE_CONTENTS }}

  upload-results-to-allure:
    needs: run-pytest
    uses: ./.github/workflows/push-results-to-allure.yml
    with:
      inputs_path: ./.ci/allure/inputs-push-results.yml
    secrets:
      ssh-key: ${{ secrets.TESTER_SSH_KEY }}
```

## `.github/workflows/ensure-infra-compose.yml`

```yaml
name: Ensure Infra + Compose (Template)
on:
  workflow_call:
    inputs:
      infra_path:
        description: "Path to unified infra spec (v2) in this repo"
        type: string
        required: true
    secrets:
      ssh-key:
        required: true

jobs:
  load:
    name: Load & normalize infra spec (v2)
    runs-on: ubuntu-latest
    outputs:
      infra_config: ${{ steps.out.outputs.infra_config }}
    steps:
      - uses: actions/checkout@v4
      - name: Load spec via Ansible (local)
        run: |
          ansible-playbook -i localhost, -c local \
            ./.ansible/playbooks/load_infra_spec.yml \
            --extra-vars "config_path=${{ github.workspace }}/${{ inputs.infra_path }}"
      - id: out
        name: Export infra_config JSON
        shell: bash
        run: |
          echo "infra_config=$(cat infra_config.json)" >> "$GITHUB_OUTPUT"

  install-python:
    name: Install Python (if requested)
    needs: load
    if: ${{ fromJSON(needs.load.outputs.infra_config).ensure.python == true }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Start SSH Agent
        uses: webfactory/ssh-agent@v0.8.0
        with:
          ssh-private-key: ${{ secrets.ssh-key }}
      - name: Ensure Python on remote
        run: |
          ansible-playbook -i ./.ansible/inventory/hosts.ini \
            --limit "${{ fromJSON(needs.load.outputs.infra_config).inventory_host }}" \
            ./.ansible/playbooks/ensure_env.yml \
            --extra-vars '{"ensure_python": true, "ensure_docker": false}'

  install-docker:
    name: Install Docker (if requested)
    needs: [load, install-python]
    if: ${{ fromJSON(needs.load.outputs.infra_config).ensure.docker == true }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Start SSH Agent
        uses: webfactory/ssh-agent@v0.8.0
        with:
          ssh-private-key: ${{ secrets.ssh-key }}
      - name: Ensure Docker on remote
        run: |
          ansible-playbook -i ./.ansible/inventory/hosts.ini \
            --limit "${{ fromJSON(needs.load.outputs.infra_config).inventory_host }}" \
            ./.ansible/playbooks/ensure_env.yml \
            --extra-vars '{"ensure_python": false, "ensure_docker": true}'

  create-directories:
    name: Create directories (if provided)
    needs: load
    if: ${{ fromJSON(needs.load.outputs.infra_config).ensure.directories && fromJSON(needs.load.outputs.infra_config).ensure.directories != '' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Start SSH Agent
        uses: webfactory/ssh-agent@v0.8.0
        with:
          ssh-private-key: ${{ secrets.ssh-key }}
      - name: Create directories
        run: |
          python - <<'PY'
import json
j='''${{ toJSON(fromJSON(needs.load.outputs.infra_config).ensure.directories) }}'''
open('directories.json','w').write(j)
PY
          ansible-playbook -i ./.ansible/inventory/hosts.ini \
            --limit "${{ fromJSON(needs.load.outputs.infra_config).inventory_host }}" \
            ./.ansible/playbooks/create_directories.yml \
            --extra-vars "@directories.json"

  upload-files:
    name: Upload files (if provided)
    needs: [load, create-directories]
    if: ${{ fromJSON(needs.load.outputs.infra_config).ensure.files && fromJSON(needs.load.outputs.infra_config).ensure.files != '' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Start SSH Agent
        uses: webfactory/ssh-agent@v0.8.0
        with:
          ssh-private-key: ${{ secrets.ssh-key }}
      - name: Upload files to remote
        run: |
          python - <<'PY'
import json
j='''${{ toJSON(fromJSON(needs.load.outputs.infra_config).ensure.files) }}'''
open('files.json','w').write(j)
PY
          ansible-playbook -i ./.ansible/inventory/hosts.ini \
            --limit "${{ fromJSON(needs.load.outputs.infra_config).inventory_host }}" \
            ./.ansible/playbooks/upload_files.yml \
            --extra-vars "@files.json"

  ensure-compose:
    name: Ensure Compose stacks (if compose_setup_path provided)
    needs: [load, install-docker, upload-files]
    if: ${{ fromJSON(needs.load.outputs.infra_config).has_compose_setup }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Start SSH Agent
        uses: webfactory/ssh-agent@v0.8.0
        with:
          ssh-private-key: ${{ secrets.ssh-key }}
      - name: Ensure Compose
        run: |
          ansible-playbook -i ./.ansible/inventory/hosts.ini \
            --limit "${{ fromJSON(needs.load.outputs.infra_config).inventory_host }}" \
            ./.ansible/playbooks/ensure_compose.yml \
            --extra-vars "setup_path=${{ fromJSON(needs.load.outputs.infra_config).compose_setup_path }}"
```

## `.github/workflows/run-pytest.yml`

```yaml
name: Run Pytest (Template)
on:
  workflow_call:
    inputs:
      spec_path:
        description: "Path to pytest spec YAML"
        type: string
        required: true
    secrets:
      ssh-key:
        required: true
      TESTER_ENV_FILE_CONTENTS:
        required: true

jobs:
  load:
    name: Load pytest spec
    runs-on: ubuntu-latest
    outputs:
      pytest_config: ${{ steps.out.outputs.pytest_config }}
    steps:
      - uses: actions/checkout@v4
      - name: Load spec via Ansible (local)
        run: |
          ansible-playbook -i localhost, -c local \
            ./.ansible/playbooks/load_pytest_spec.yml \
            --extra-vars "spec_path=${{ github.workspace }}/${{ inputs.spec_path }}"
      - id: out
        name: Export pytest_config JSON
        shell: bash
        run: |
          echo "pytest_config=$(cat pytest_config.json)" >> "$GITHUB_OUTPUT"

  run:
    name: Execute pytest on remote
    needs: load
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Start SSH Agent
        uses: webfactory/ssh-agent@v0.8.0
        with:
          ssh-private-key: ${{ secrets.ssh-key }}
      - name: Run pytest via Ansible
        env:
          PYTEST_CONFIG: ${{ needs.load.outputs.pytest_config }}
          TESTER_ENV_FILE_CONTENTS: ${{ secrets.TESTER_ENV_FILE_CONTENTS }}
        shell: bash
        run: |
          printf '%s' "$PYTEST_CONFIG" > pytest_config.json
          python - <<'PY'
import json
cfg=json.load(open('pytest_config.json'))
open('pytest_vars.json','w').write(json.dumps({
  'pytest_config': cfg,
  'env_file_contents': "${{ secrets.TESTER_ENV_FILE_CONTENTS }}"
}))
PY
          ansible-playbook -i ./.ansible/inventory/hosts.ini \
            --limit "$(jq -r .inventory_host < pytest_config.json)" \
            ./.ansible/playbooks/run_pytest.yml \
            --extra-vars "@pytest_vars.json"
      - uses: actions/upload-artifact@v4
        with:
          name: raw-results
          path: raw-results-*.tgz
          if-no-files-found: error
```

## `.github/workflows/push-results-to-allure.yml`

```yaml
name: Push Results to Allure (Template)
on:
  workflow_call:
    inputs:
      inputs_path:
        description: "Path to inputs-push-results.yml"
        type: string
        required: true
    secrets:
      ssh-key:
        required: true

jobs:
  load:
    name: Load inputs file
    runs-on: ubuntu-latest
    outputs:
      push_inputs: ${{ steps.out.outputs.push_inputs }}
    steps:
      - uses: actions/checkout@v4
      - name: Load inputs via Ansible (local)
        run: |
          ansible-playbook -i localhost, -c local \
            ./.ansible/playbooks/load_pytest_spec.yml \
            --extra-vars "spec_path=${{ github.workspace }}/${{ inputs.inputs_path }}"
          mv pytest_config.json push_inputs.json
      - id: out
        name: Export push_inputs JSON
        shell: bash
        run: |
          echo "push_inputs=$(cat push_inputs.json)" >> "$GITHUB_OUTPUT"

  push-results-to-allure:
    name: Publish to Allure
    needs: load
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Start SSH Agent
        uses: webfactory/ssh-agent@v0.8.0
        with:
          ssh-private-key: ${{ secrets.ssh-key }}
      - name: Build vars.json
        run: |
          cat > vars.json <<'JSON'
          {
            "inventory_host": "${{ fromJSON(needs.load.outputs.push_inputs).inventory_host }}",
            "working_dir":   "${{ fromJSON(needs.load.outputs.push_inputs).working_dir }}",
            "results_dir":   "${{ fromJSON(needs.load.outputs.push_inputs).results_dir }}",
            "allure_url":    "${{ fromJSON(needs.load.outputs.push_inputs).allure_url }}",
            "project_id":    "${{ fromJSON(needs.load.outputs.push_inputs).project_id }}",
            "force_project_creation": ${{ fromJSON(needs.load.outputs.push_inputs).force_project_creation }},
            "generate_report": ${{ fromJSON(needs.load.outputs.push_inputs).generate_report }}
          }
          JSON
      - name: Push via Ansible
        run: |
          ansible-playbook -i ./.ansible/inventory/hosts.ini \
            --limit "$(jq -r .inventory_host vars.json)" \
            ./.ansible/playbooks/push_allure_results.yml \
            --extra-vars "@vars.json"
```

---

## Ansible playbooks (stubs)

### `.ansible/playbooks/load_infra_spec.yml`

```yaml
- hosts: localhost
  gather_facts: false
  vars:
    config_path: "{{ config_path | default('./.ci/infra/infra.compose.template.yml') }}"
  tasks:
    - name: Read infra spec YAML
      include_vars:
        file: "{{ config_path }}"
        name: cfg
    - name: Write infra_config JSON for workflow
      copy:
        content: "{{ cfg | to_json }}"
        dest: "infra_config.json"
```

### `.ansible/playbooks/load_pytest_spec.yml`

```yaml
- hosts: localhost
  gather_facts: false
  vars:
    spec_path: "{{ spec_path | default('./.ci/pytest/pytest.template.yml') }}"
  tasks:
    - name: Read spec YAML
      include_vars:
        file: "{{ spec_path }}"
        name: spec
    - name: Write JSON for workflow
      copy:
        content: "{{ spec | to_json }}"
        dest: "pytest_config.json"
```

### `.ansible/playbooks/ensure_env.yml`

```yaml
- hosts: all
  become: true
  gather_facts: false
  vars:
    ensure_python: false
    ensure_docker: false
  tasks:
    - name: Ensure Python packages (Debian/Ubuntu)
      apt:
        name: [python3, python3-venv, python3-pip]
        state: present
        update_cache: true
      when: ensure_python | bool

    - name: Install Docker (script convenience)  # use hardened method in prod
      shell: |
        curl -fsSL https://get.docker.com | sh
      args:
        warn: false
      when: ensure_docker | bool
```

### `.ansible/playbooks/create_directories.yml`

```yaml
- hosts: all
  become: true
  gather_facts: false
  vars:
    _dirs: []
  tasks:
    - name: Load directories JSON from extra-vars
      set_fact:
        _dirs: "{{ _dirs if _dirs else (directories_json | default([])) }}"
    - name: Create directories idempotently
      file:
        path: "{{ item }}"
        state: directory
        mode: "0755"
      loop: "{{ _dirs }}"
```

### `.ansible/playbooks/upload_files.yml`

```yaml
- hosts: all
  become: true
  gather_facts: false
  vars:
    files_json: []
  tasks:
    - name: Ensure destination dirs exist
      file:
        path: "{{ item.dest | dirname }}"
        state: directory
        mode: "0755"
      loop: "{{ files_json }}"
    - name: Upload files from repo to remote
      copy:
        src: "{{ item.src }}"
        dest: "{{ item.dest }}"
        mode: "0644"
      loop: "{{ files_json }}"
```

### `.ansible/playbooks/ensure_compose.yml`

```yaml
- hosts: all
  become: true
  gather_facts: false
  vars:
    setup_path: ""
  tasks:
    - name: Ensure docker compose up -d
      shell: |
        set -euo pipefail
        cd "{{ setup_path }}"
        docker compose up -d
```

### `.ansible/playbooks/run_pytest.yml`

```yaml
- hosts: all
  become: false
  gather_facts: false
  vars:
    pytest_config: {}
    env_file_contents: ""
  tasks:
    - name: Extract config vars
      set_fact:
        wd: "{{ pytest_config.working_dir }}"
        venv: "{{ pytest_config.pytest.venv_path }}"
        cmd: "{{ pytest_config.pytest.test_command }}"
        out_tgz: "{{ pytest_config.artifacts.output_tgz | default('raw-results-template.tgz') }}"
    - name: Prepare working dir
      file:
        path: "{{ wd }}"
        state: directory
        mode: "0755"
    - name: Create .env for tests (if provided)
      copy:
        content: "{{ env_file_contents }}"
        dest: "{{ wd }}/.env"
      when: env_file_contents | length > 0
    - name: Ensure venv and pytest
      shell: |
        set -e
        python3 -m venv "{{ venv }}"
        "{{ venv }}/bin/pip" install -U pip pytest
      args: { chdir: "{{ wd }}" }
    - name: Run tests
      shell: |
        set -e
        . "{{ venv }}/bin/activate"
        mkdir -p raw-results
        {{ cmd }}
        echo "sample" > raw-results/sample.txt
      args: { chdir: "{{ wd }}" }
    - name: Tar raw-results for upload
      shell: |
        set -e
        cd "{{ wd }}"
        tar -czf "{{ out_tgz }}" raw-results
        cp "{{ out_tgz }}" "{{ lookup('env','GITHUB_WORKSPACE') | default('.') }}/"
```

### `.ansible/playbooks/push_allure_results.yml`

```yaml
- hosts: all
  gather_facts: false
  vars:
    working_dir: ""
    results_dir: "raw-results"
    allure_url: ""
    project_id: ""
    force_project_creation: false
    generate_report: true
  tasks:
    - name: Zip results directory
      shell: |
        set -e
        cd "{{ working_dir }}"
        tar -czf /tmp/allure-results.tar.gz "{{ results_dir }}"
    - name: Create project (optional)
      uri:
        url: "{{ allure_url }}/allure-docker-service/create-project?project_id={{ project_id }}"
        method: POST
        status_code: 200,409
      when: force_project_creation | bool
    - name: Send results
      uri:
        url: "{{ allure_url }}/allure-docker-service/send-results?project_id={{ project_id }}"
        method: POST
        body_format: form-multipart
        body:
          files[]: "@/tmp/allure-results.tar.gz"
      register: send_resp
    - name: Generate report (optional)
      uri:
        url: "{{ allure_url }}/allure-docker-service/generate-report?project_id={{ project_id }}"
        method: GET
      when: generate_report | bool
```

---

## Specs & docs

### `.ansible/inventory/hosts.ini`

```ini
# DO NOT COMMIT REAL HOSTS/IPs IN PUBLIC REPOS
[tester]
<INVENTORY_HOST> ansible_user=<SSH_USER>
```

### `.ci/infra/infra.compose.template.yml`

```yaml
version: 2
inventory_host: "<INVENTORY_HOST>"
ensure:
  python: true
  docker: true
  directories:
    - "/var/tmp/test-results"
  files:
    - src: ".ci/payloads/synthetic_flux_source.log"
      dest: "/var/tmp/synthetic_flux_source.log"
compose_setup_path: ".ci/compose/setup"
has_compose_setup: true
any_clone: false
clone_specs: []
```

### `.ci/pytest/pytest.template.yml`

```yaml
inventory_host: "<INVENTORY_HOST>"
working_dir: "/opt/tests"
pytest:
  venv_path: "/opt/venv"
  test_command: "pytest -q tests/test_smoke.py"
artifacts:
  output_tgz: "raw-results-template.tgz"
```

### `.ci/allure/inputs-push-results.yml`

```yaml
inventory_host: "<INVENTORY_HOST>"
working_dir: "/opt/tests"
results_dir: "raw-results"
allure_url: "http://<ALLURE_VM_HOST>:5050"
project_id: "example-project"
force_project_creation: true
generate_report: true
```

### `docs/allure-server-setup.md`

```md
(Host Allure on a separate VM using allure-docker-service; sample docker-compose; API endpoints; security notes.)
```

---

## What to change before making the repo public

* Replace host/IP with placeholders and keep inventory private.
* Keep secrets in GitHub Secrets only (SSH, `.env`, tokens).
* Remove any logs/artifacts that might contain sensitive data.
* Optional: add a small `tests/test_smoke.py` to demo pytest.

---

If you want, I can also add a minimal `tests/test_smoke.py`, a small `compose` folder, and a `Makefile` with helper targets (`bootstrap`, `run`, `publish`).
